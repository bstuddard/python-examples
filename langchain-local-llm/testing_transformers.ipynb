{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64ac4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.models.smollm3.modeling_smollm3 import SmolLM3ForCausalLM\n",
    "from transformers.tokenization_utils_fast import PreTrainedTokenizerFast\n",
    "import torch\n",
    "from langchain_core.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langchain_core.messages import BaseMessage, AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.outputs import ChatGeneration, ChatResult\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from typing import Optional, Any, Union\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576bd151",
   "metadata": {},
   "source": [
    "## Model Download\n",
    "Downloading models via hugging face transformers and loading to gpu. Testing out a simple prompt that matches the docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5ccbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a models directory in your project\n",
    "project_models_dir = os.path.join(os.getcwd(), \"models\", \"SmolLM3-3B-transformers\")\n",
    "os.makedirs(project_models_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aadb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = \"HuggingFaceTB/SmolLM3-3B\"\n",
    "device = \"cuda\"  # for GPU usage or \"cpu\" for CPU usage\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=project_models_dir\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=project_models_dir\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42ccad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the setup\n",
    "print(f\"Model cached in: {project_models_dir}\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961cc699",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model type: {type(model)}\")\n",
    "print(f\"Tokenizer type: {type(tokenizer)}\")\n",
    "print(f\"Model module: {type(model).__module__}\")\n",
    "print(f\"Tokenizer module: {type(tokenizer).__module__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36f77fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the model input\n",
    "prompt = \"Give me a brief explanation of gravity in simple terms.\"\n",
    "messages_think = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages_think,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "print(f'Text: {text}')\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate the output\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=32768)\n",
    "\n",
    "# Get and decode the output\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]) :]\n",
    "print(tokenizer.decode(output_ids, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299d4afe",
   "metadata": {},
   "source": [
    "## Custom Langchain Chat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a9f31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmolLM3LLM(BaseChatModel):\n",
    "    model: SmolLM3ForCausalLM\n",
    "    tokenizer: PreTrainedTokenizerFast\n",
    "    max_new_tokens: int = 512\n",
    "    temperature: float = 0.6\n",
    "    top_p: float = 0.95\n",
    "    model_name: str = \"SmolLM3-3B\"\n",
    "\n",
    "    \n",
    "    def __init__(self, model: SmolLM3ForCausalLM, tokenizer: PreTrainedTokenizerFast, **kwargs):\n",
    "        super().__init__(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            **kwargs\n",
    "        )\n",
    "        self._structured_output_parser: Optional[PydanticOutputParser] = None\n",
    "        self._structured_output_schema = None\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Get the type of language model used by this chat model.\n",
    "        \n",
    "        Used by LangChain for logging and monitoring purposes.\n",
    "        \"\"\"\n",
    "        return \"smollm3-chat-model\"\n",
    "\n",
    "    \n",
    "    @property\n",
    "    def _identifying_params(self) -> dict[str, Any]:\n",
    "        \"\"\"Return a dictionary of identifying parameters.\n",
    "\n",
    "        This information is used by the LangChain callback system, which\n",
    "        is used for tracing purposes make it possible to monitor LLMs.\n",
    "        \n",
    "        Returns:\n",
    "            Dict containing model identification parameters including\n",
    "            model name for custom token counting rules in LLM monitoring\n",
    "            applications (e.g., in LangSmith users can provide per token \n",
    "            pricing for their model and monitor costs for the given LLM.)\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"model_name\": self.model_name,\n",
    "            \"max_new_tokens\": self.max_new_tokens,\n",
    "            \"temperature\": self.temperature,\n",
    "            \"top_p\": self.top_p,\n",
    "            \"model_type\": \"SmolLM3ForCausalLM\",\n",
    "            \"torch_dtype\": str(self.model.dtype) if hasattr(self.model, 'dtype') else \"unknown\",\n",
    "            \"device\": str(self.model.device) if hasattr(self.model, 'device') else \"unknown\"\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def _extract_json(self, message: AIMessage, output_parser: PydanticOutputParser) -> Any:\n",
    "        \"\"\"Extracts JSON content from a string where JSON is embedded between \\`\\`\\`json and \\`\\`\\` tags.\n",
    "\n",
    "        Parameters:\n",
    "            text (str): The text containing the JSON content.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of extracted JSON strings.\n",
    "        \"\"\"\n",
    "        text = message.content\n",
    "        # Define the regular expression pattern to match JSON blocks\n",
    "        pattern = r\"\\`\\`\\`json(.*?)\\`\\`\\`\"\n",
    "\n",
    "        # Find all non-overlapping matches of the pattern in the string\n",
    "        matches = re.findall(pattern, text, re.DOTALL)\n",
    "\n",
    "        # Return the list of matched JSON strings, stripping any leading or trailing whitespace\n",
    "        try:\n",
    "            result = [json.loads(match.strip()) for match in matches]\n",
    "        except Exception:\n",
    "            raise ValueError(f\"Failed to parse: {message}\")\n",
    "        \n",
    "        return output_parser.pydantic_object.model_validate(result[0]['properties'])\n",
    "    \n",
    "    \n",
    "    def with_structured_output(\n",
    "        self,\n",
    "        schema: Union[dict, BaseModel]\n",
    "    ) -> BaseChatModel:\n",
    "        new_model = self.__class__(\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_new_tokens=self.max_new_tokens,\n",
    "            temperature=self.temperature,\n",
    "            top_p=self.top_p,\n",
    "            model_name=self.model_name\n",
    "        )\n",
    "        new_model._structured_output_parser = PydanticOutputParser(pydantic_object=schema)\n",
    "        new_model._structured_output_schema = schema\n",
    "        return new_model\n",
    "    \n",
    "    \n",
    "    def _generate(\n",
    "            self, \n",
    "            messages: list[BaseMessage], \n",
    "            stop: Optional[list[str]] = None,\n",
    "            run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "            **kwargs\n",
    "    ) -> ChatResult:\n",
    "        start_time = time.time()\n",
    "\n",
    "        # prepare the model input\n",
    "        chat_messages = []\n",
    "        for message in messages:\n",
    "            if isinstance(message, HumanMessage):\n",
    "                chat_messages.append({\"role\": \"user\", \"content\": message.content})\n",
    "            elif isinstance(message, AIMessage):\n",
    "                chat_messages.append({\"role\": \"assistant\", \"content\": message.content})\n",
    "            elif isinstance(message, SystemMessage):\n",
    "                chat_messages.append({\"role\": \"system\", \"content\": message.content})\n",
    "\n",
    "        # Append instructions if structured output\n",
    "        if self._structured_output_parser is not None:\n",
    "            format_instructions = self._structured_output_parser.get_format_instructions()\n",
    "            system_message_found = False\n",
    "            for i, msg in enumerate(chat_messages):\n",
    "                if msg[\"role\"] == \"system\":\n",
    "                    # Append format instructions to existing system message\n",
    "                    chat_messages[i][\"content\"] = f\"{msg['content']}\\n\\nPlease format your response as JSON wrapped in ```json tags.\\n{format_instructions}/no_think\"\n",
    "                    system_message_found = True\n",
    "                    break\n",
    "            if not system_message_found:\n",
    "                system_prompt = f\"Please format your response as JSON wrapped in ```json tags.\\n{format_instructions}/no_think\"\n",
    "                chat_messages.insert(0, {\"role\": \"system\", \"content\": system_prompt})\n",
    "\n",
    "        text = self.tokenizer.apply_chat_template(\n",
    "            chat_messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "        model_inputs = self.tokenizer([text], return_tensors=\"pt\").to(self.model.device)\n",
    "        input_token_count = model_inputs.input_ids.shape[1]\n",
    "\n",
    "        # Generate the output\n",
    "        generated_ids = self.model.generate(\n",
    "            **model_inputs, \n",
    "            max_new_tokens=self.max_new_tokens, \n",
    "            temperature=self.temperature, \n",
    "            top_p=self.top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=self.tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "        # Get and decode the output\n",
    "        output_ids = generated_ids[0][len(model_inputs.input_ids[0]) :]\n",
    "        response = self.tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "        parsed_response = None\n",
    "        if self._structured_output_parser is not None:\n",
    "            try:\n",
    "                parsed_response = self._extract_json(AIMessage(response),self._structured_output_parser)\n",
    "            except:\n",
    "                parsed_response = None\n",
    "\n",
    "        # Calculate timing and token counts\n",
    "        end_time = time.time()\n",
    "        generation_time = end_time - start_time\n",
    "        output_token_count = len(output_ids)\n",
    "        total_token_count = input_token_count + output_token_count\n",
    "        \n",
    "\n",
    "        # Return as chat result\n",
    "        message = AIMessage(\n",
    "            content=response,\n",
    "            additional_kwargs={\n",
    "                \"parsed_response\": parsed_response\n",
    "            },\n",
    "            response_metadata={\n",
    "                \"time_in_seconds\": generation_time,\n",
    "                \"model_name\": self.model_name,\n",
    "                \"finish_reason\": \"stop\"\n",
    "            },\n",
    "            usage_metadata={\n",
    "                \"input_tokens\": input_token_count,\n",
    "                \"output_tokens\": output_token_count,\n",
    "                \"total_tokens\": total_token_count,\n",
    "            }\n",
    "        )\n",
    "        generation = ChatGeneration(message=message)\n",
    "        return ChatResult(generations=[generation])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc2aec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = SmolLM3LLM(model=model, tokenizer=tokenizer)\n",
    "response = llm.invoke(\"Give me a brief explanation of gravity in simple terms.\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfe94b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm = SmolLM3LLM(model=model, tokenizer=tokenizer)\n",
    "response = llm.invoke([\n",
    "    HumanMessage(content=\"What is your name?\"),\n",
    "    AIMessage(content=\"Jack\"),\n",
    "    HumanMessage(content=\"I missed it, what was your name?\"),\n",
    "])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4714c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Joke(BaseModel):\n",
    "    \"\"\"Joke to tell user.\"\"\"\n",
    "\n",
    "    setup: str = Field(description=\"The setup of the joke\")\n",
    "    punchline: str = Field(description=\"The punchline to the joke\")\n",
    "    rating: Optional[int] = Field(\n",
    "        default=None, description=\"How funny the joke is, from 1 to 10\"\n",
    "    )\n",
    "\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Joke)\n",
    "llm = SmolLM3LLM(model=model, tokenizer=tokenizer)\n",
    "structured_llm = llm.with_structured_output(Joke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca356cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = structured_llm.invoke([\n",
    "    HumanMessage(content=\"Tell me a joke\")\n",
    "])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4dae5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.additional_kwargs['parsed_response']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local_llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
