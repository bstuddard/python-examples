{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing - Tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports \n",
    "There are many tokenizers out there. For this we'll try just a few."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import torchtext\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Tokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from torchtext.data import get_tokenizer\n",
    "\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# Stopwords\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# Punctuation\n",
    "import string\n",
    "\n",
    "# Lemmetize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "# Stem\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Word2vec\n",
    "import gensim.downloader\n",
    "\n",
    "print('Libraries imported and NLTK data downloaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "For the dataset example, we'll be using some lyrics from Jimi Hendrix's song The Wind Cries Mary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('the_wind_cries_mary.txt', 'r') as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['After all the jacks are in their boxes\\n',\n",
       " 'And the clowns have all gone to bed\\n',\n",
       " 'You can hear happiness staggering on down the street\\n',\n",
       " 'Footprints dressed in red\\n',\n",
       " 'And the wind whispers Mary \\n',\n",
       " 'A broom is drearily sweeping\\n',\n",
       " 'Up the broken pieces of yesterdays life\\n',\n",
       " 'Somewhere a queen is weeping\\n',\n",
       " 'Somewhere a king has no wife\\n',\n",
       " 'And the wind, it cries Mary \\n',\n",
       " 'The traffic lights they all true blue tomorrow\\n',\n",
       " 'And shine their emptiness down on my bed\\n',\n",
       " 'The tiny island sags downstream\\n',\n",
       " 'Cause the life that lived is, is dead\\n',\n",
       " 'And the wind screams Mary \\n',\n",
       " 'Will the wind ever remember\\n',\n",
       " 'The names it has blown in the past\\n',\n",
       " \"And with his crutch, its old age, and it's wisdom\\n\",\n",
       " 'It whispers no, this will be the last\\n',\n",
       " 'And the wind cries Mary \\n']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_lines = lines[5:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A broom is drearily sweeping\\n',\n",
       " 'Up the broken pieces of yesterdays life\\n',\n",
       " 'Somewhere a queen is weeping\\n',\n",
       " 'Somewhere a king has no wife\\n',\n",
       " 'And the wind, it cries Mary \\n']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize with NLTK\n",
    "https://www.nltk.org/api/nltk.tokenize.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['A', 'broom', 'is', 'drearily', 'sweeping'],\n",
       " ['Up', 'the', 'broken', 'pieces', 'of', 'yesterdays', 'life'],\n",
       " ['Somewhere', 'a', 'queen', 'is', 'weeping'],\n",
       " ['Somewhere', 'a', 'king', 'has', 'no', 'wife'],\n",
       " ['And', 'the', 'wind', ',', 'it', 'cries', 'Mary']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_lines_tokenized = [word_tokenize(line) for line in sample_lines]\n",
    "sample_lines_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize with Pytorch\n",
    "https://pytorch.org/text/stable/_modules/torchtext/data/utils.html#get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a', 'broom', 'is', 'drearily', 'sweeping'],\n",
       " ['up', 'the', 'broken', 'pieces', 'of', 'yesterdays', 'life'],\n",
       " ['somewhere', 'a', 'queen', 'is', 'weeping'],\n",
       " ['somewhere', 'a', 'king', 'has', 'no', 'wife'],\n",
       " ['and', 'the', 'wind', ',', 'it', 'cries', 'mary']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_tokenizer = get_tokenizer(\"basic_english\")\n",
    "pytorch_tokens = [pytorch_tokenizer(line) for line in sample_lines]\n",
    "pytorch_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['A', 'broom', 'is', 'drearily', 'sweeping'],\n",
       " ['Up', 'the', 'broken', 'pieces', 'of', 'yesterdays', 'life'],\n",
       " ['Somewhere', 'a', 'queen', 'is', 'weeping'],\n",
       " ['Somewhere', 'a', 'king', 'has', 'no', 'wife'],\n",
       " ['And', 'the', 'wind', ',', 'it', 'cries', 'Mary']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK from pytorch\n",
    "pytorch_tokenizer_toktok = get_tokenizer('toktok')\n",
    "pytorch_tokens_toktok = [pytorch_tokenizer_toktok(line) for line in sample_lines]\n",
    "pytorch_tokens_toktok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize with Spacy\n",
    "https://spacy.io/api/tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[A broom is drearily sweeping,\n",
       " Up the broken pieces of yesterdays life,\n",
       " Somewhere a queen is weeping,\n",
       " Somewhere a king has no wife,\n",
       " And the wind, it cries Mary ]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = English()\n",
    "spacy_tokenizer = Tokenizer(nlp.vocab)\n",
    "spacy_tokens = [spacy_tokenizer(line) for line in sample_lines]\n",
    "spacy_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spacy returns a doc object, not just list of words. See more here: https://spacy.io/api/doc\n",
    "type(spacy_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['A', 'broom', 'is', 'drearily', 'sweeping', '\\n'],\n",
       " ['Up', 'the', 'broken', 'pieces', 'of', 'yesterdays', 'life', '\\n'],\n",
       " ['Somewhere', 'a', 'queen', 'is', 'weeping', '\\n'],\n",
       " ['Somewhere', 'a', 'king', 'has', 'no', 'wife', '\\n'],\n",
       " ['And', 'the', 'wind,', 'it', 'cries', 'Mary', '\\n']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[token.text for token in line] for line in spacy_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# View stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['~A~', 'broom', '~is~', 'drearily', 'sweeping'],\n",
       " ['~Up~', '~the~', 'broken', 'pieces', '~of~', 'yesterdays', 'life'],\n",
       " ['Somewhere', '~a~', 'queen', '~is~', 'weeping'],\n",
       " ['Somewhere', '~a~', 'king', '~has~', '~no~', 'wife'],\n",
       " ['~And~', '~the~', 'wind', ',', '~it~', 'cries', 'Mary']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find all stopwords and highlight with \"~\"\n",
    "mod_tokens = []\n",
    "for i, line in enumerate(sample_lines_tokenized):\n",
    "    mod_line = []\n",
    "    for token in line:\n",
    "        if token.lower() in stopwords.words('english'):\n",
    "            mod_line.append(f'~{token}~')\n",
    "        else:\n",
    "            mod_line.append(token)\n",
    "    mod_tokens.append(mod_line)\n",
    "mod_tokens\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(input_text):\n",
    "    return [token for token in input_text if token.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['broom', 'drearily', 'sweeping'],\n",
       " ['broken', 'pieces', 'yesterdays', 'life'],\n",
       " ['Somewhere', 'queen', 'weeping'],\n",
       " ['Somewhere', 'king', 'wife'],\n",
       " ['wind', ',', 'cries', 'Mary']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_without_stopwords = [remove_stopwords(line) for line in sample_lines_tokenized]\n",
    "tokens_without_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[')', '(', '>', '|', '!']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation_characters = set(string.punctuation)\n",
    "list(punctuation_characters)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(input_text):\n",
    "    return [token for token in input_text if token not in set(string.punctuation)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['broom', 'drearily', 'sweeping'],\n",
       " ['broken', 'pieces', 'yesterdays', 'life'],\n",
       " ['Somewhere', 'queen', 'weeping'],\n",
       " ['Somewhere', 'king', 'wife'],\n",
       " ['wind', 'cries', 'Mary']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_without_punctuation = [remove_punctuation(line) for line in tokens_without_stopwords]\n",
    "tokens_without_punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmetize\n",
    "https://www.nltk.org/api/nltk.stem.wordnet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cry'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem = WordNetLemmatizer()\n",
    "lem.lemmatize('cries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(input_text):\n",
    "    # Instantiate class\n",
    "    lem = WordNetLemmatizer()\n",
    "    # Lemmatized text becomes input inside all loop runs\n",
    "    lemmatized_text = input_text\n",
    "    # Lemmatize each part of speech\n",
    "    for part_of_speech in ['n', 'v', 'a', 'r', 's']:\n",
    "        lemmatized_text = [lem.lemmatize(token, part_of_speech).lower() for token in lemmatized_text]\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['broom', 'drearily', 'sweep'],\n",
       " ['break', 'piece', 'yesterday', 'life'],\n",
       " ['somewhere', 'queen', 'weep'],\n",
       " ['somewhere', 'king', 'wife'],\n",
       " ['wind', 'cry', 'mary']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_lemmatized = [lemmatize(line) for line in tokens_without_punctuation]\n",
    "tokens_lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stem\n",
    "https://www.nltk.org/api/nltk.stem.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sweep'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "stemmer.stem('sweeping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(input_text):\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    return [stemmer.stem(token) for token in input_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['broom', 'drearili', 'sweep'],\n",
       " ['broken', 'piec', 'yesterday', 'life'],\n",
       " ['somewher', 'queen', 'weep'],\n",
       " ['somewher', 'king', 'wife'],\n",
       " ['wind', 'cri', 'mari']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_stemmed = [stem(line) for line in tokens_without_punctuation]\n",
    "tokens_stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_list_of_text(\n",
    "        input_text, \n",
    "        enable_stopword_removal=True,\n",
    "        enable_punctuation_removal=True,\n",
    "        enable_lemmatization=True,\n",
    "        enable_stemming=False\n",
    "    ):\n",
    "    # Get list of operations\n",
    "    enabled_operations = [word_tokenize]\n",
    "    if enable_stopword_removal:\n",
    "        enabled_operations.append(remove_stopwords)\n",
    "    if enable_punctuation_removal:\n",
    "        enabled_operations.append(remove_punctuation)\n",
    "    if enable_lemmatization:\n",
    "        enabled_operations.append(lemmatize)\n",
    "    if enable_stemming:\n",
    "        enabled_operations.append(stem)\n",
    "    print(f'Enabled Operations: {len(enabled_operations)}')\n",
    "    \n",
    "\n",
    "    # Run all operations\n",
    "    cleaned_text_lines = input_text\n",
    "    for operation in enabled_operations:\n",
    "        # Run for all lines\n",
    "        cleaned_text_lines = [operation(line) for line in cleaned_text_lines]\n",
    "    \n",
    "    return cleaned_text_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabled Operations: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['broom', 'drearily', 'sweep'],\n",
       " ['break', 'piece', 'yesterday', 'life'],\n",
       " ['somewhere', 'queen', 'weep'],\n",
       " ['somewhere', 'king', 'wife'],\n",
       " ['wind', 'cry', 'mary']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_list_of_text(sample_lines, enable_stopword_removal=True, enable_punctuation_removal=True, enable_lemmatization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec and Glove Embedding\n",
    "https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained gensim model\n",
    "glove_model = gensim.downloader.load(\"glove-wiki-gigaword-100\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('princess', 0.7947244644165039),\n",
       " ('king', 0.7507690191268921),\n",
       " ('elizabeth', 0.7355712056159973),\n",
       " ('royal', 0.7065026164054871),\n",
       " ('lady', 0.7044796943664551),\n",
       " ('victoria', 0.6853757500648499),\n",
       " ('monarch', 0.6683257222175598),\n",
       " ('crown', 0.6680562496185303),\n",
       " ('prince', 0.6640505194664001),\n",
       " ('consort', 0.6570538282394409)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show default most similar words given a word\n",
    "glove_model.most_similar('queen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show embed size\n",
    "glove_model['broom'].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.20009   ,  0.32409   , -0.23066   , -0.61079   , -0.42757   ,\n",
       "        0.0020605 , -0.45512   ,  0.56479   , -0.55531   , -0.25579   ,\n",
       "       -0.72523   ,  0.55213   , -0.19549   ,  0.96065   , -0.55447   ,\n",
       "        0.68811   ,  0.039949  ,  0.47085   , -0.45799   , -0.74935   ,\n",
       "       -0.39437   ,  0.25289   ,  1.0068    , -0.66637   ,  0.63259   ,\n",
       "        1.0547    , -0.14611   ,  0.35851   , -0.25193   , -0.023974  ,\n",
       "        0.26526   ,  0.056152  ,  0.27812   ,  0.14538   ,  0.12781   ,\n",
       "        0.30503   ,  0.024989  , -0.47947   ,  1.2966    ,  0.032496  ,\n",
       "       -0.25516   , -0.39946   ,  0.22301   , -0.74436   , -0.46208   ,\n",
       "        0.20526   ,  0.14991   , -0.36987   ,  0.27937   , -0.014941  ,\n",
       "       -0.85951   , -0.24261   , -0.33566   ,  0.71803   , -0.86      ,\n",
       "       -0.60147   , -0.84878   ,  0.12036   ,  0.21095   , -0.54984   ,\n",
       "        0.32912   , -0.24656   ,  0.13614   ,  0.52457   ,  0.31397   ,\n",
       "       -0.26055   ,  0.29517   , -0.15317   ,  0.33613   , -0.068772  ,\n",
       "       -0.48816   , -0.089065  , -0.0023651 ,  0.005977  , -0.044743  ,\n",
       "       -0.042121  , -0.30449   , -0.2501    ,  0.28114   ,  0.33302   ,\n",
       "        0.2316    ,  0.024338  , -0.040168  ,  0.41974   ,  0.12671   ,\n",
       "        0.052348  ,  0.53306   ,  0.049745  , -0.097174  ,  0.68151   ,\n",
       "       -0.37324   ,  0.0043775 ,  0.46469   ,  0.15618   ,  1.2232    ,\n",
       "       -1.002     , -0.085826  , -0.074463  , -0.50526   ,  0.00072131],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# Sample of embedding vector for a word\n",
    "glove_model['broom']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabled Operations: 4\n"
     ]
    }
   ],
   "source": [
    "# Example of grabbing embedding for each word\n",
    "text_to_convert = clean_list_of_text(sample_lines, enable_stopword_removal=True, enable_punctuation_removal=True, enable_lemmatization=True)\n",
    "vectors = [[glove_model[token] for token in line] for line in text_to_convert]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4, 100])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of padding those embeddings and converting to torch tensor (num_examples, sequence_length, embed_dim)\n",
    "torch_padded_tensor = torch.nn.utils.rnn.pad_sequence([torch.FloatTensor(vector) for vector in vectors], batch_first=True)\n",
    "torch_padded_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defaults to zero for padded values\n",
    "torch_padded_tensor[0][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache\\glove.6B.zip: 862MB [04:24, 3.26MB/s]                           \n",
      "100%|█████████▉| 399999/400000 [00:11<00:00, 34874.24it/s]\n"
     ]
    }
   ],
   "source": [
    "vec = torchtext.vocab.GloVe(name='6B', dim=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2070, -0.3793, -0.8425, -0.2447,  0.4185,  0.6902,  0.4083, -0.0413,\n",
       "          0.7953, -0.4586, -0.4328,  0.7068, -0.4015,  0.4556,  0.0159,  0.4423,\n",
       "          0.7442,  0.5907, -0.4703, -1.1006,  0.7969,  0.0286,  0.4297,  0.2820,\n",
       "         -0.1548, -0.5966,  0.2890,  0.5291,  0.6385, -0.0024, -0.3283, -0.6784,\n",
       "         -1.1181,  0.7954,  0.2005,  0.2453, -0.1501, -0.1612,  0.7281, -0.1558,\n",
       "          0.1464, -0.3415, -0.1887,  0.6934,  0.8386,  0.1495,  0.6169, -1.2661,\n",
       "         -0.0847, -0.5917]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.get_vecs_by_tokens(['broom'], lower_case_backup=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "557bea64a4d33a54b4028138dfa5f7ef28b1855e4803cd7cef7b3f2a14105b37"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('sliced2': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
