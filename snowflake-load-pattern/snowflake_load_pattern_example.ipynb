{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "cziarxqsmexvcjedrsjo",
   "authorId": "3749378865462",
   "authorName": "STARLIGHTMACHINELEARNING",
   "authorEmail": "starlightmachinelearning@gmail.com",
   "sessionId": "500703f3-0940-492c-b8c8-a7e32f1d1f44",
   "lastEditTime": 1759423918595
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69c4e0cc-7086-46cd-bdae-cc72205d9d06",
   "metadata": {
    "name": "overview_notes",
    "collapsed": false
   },
   "source": "# Overview\nI'm mostly familiar with databricks; however, wanted to try out building a common etl pattern in snowflake as a learning experience.\n\nThis is a basic etl pattern for fact and dimension table loads (excluding aggregates). This will assume 1 dimension and 1 fact, but the pattern is extensible to handle many tables. The only requirement would be from a scheduling standpoint dims run first and then facts based on dependency order.\n\nWhile the full example is contained in this notebook, this would be split up into multiple notebooks in a true production environment. For example:\n1. A notebook per table with just the create statements and any primary key assignments (e.g. create_dim_employee).\n2. A notebook per table with just the etl load logic per table (e.g. load_dim_employee).\n3. Various stage load processes - excluded for this demo.\n4. Common functions split out into a reusable and sharable file."
  },
  {
   "cell_type": "markdown",
   "id": "c884b2cf-5e29-4b72-ba5c-d117de712188",
   "metadata": {
    "name": "libraries_and_imports_heading",
    "collapsed": false
   },
   "source": "## Libraries and Imports"
  },
  {
   "cell_type": "code",
   "id": "34ef1041-33da-4860-883a-39f4c577a544",
   "metadata": {
    "language": "python",
    "name": "imports"
   },
   "outputs": [],
   "source": "from snowflake.snowpark.context import get_active_session\nfrom snowflake.snowpark.row import Row\nsession = get_active_session()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0dffec33-c0c1-49f0-8b59-8f317d9bc1a2",
   "metadata": {
    "language": "python",
    "name": "dynamic_database_schemas",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "database_name = 'learning_db'\nschema_name = 'dw'\nsrc_schema_name = 'src'\netl_schema_name = 'etl'",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cd697955-2301-44b8-a775-ee81c3b38c8c",
   "metadata": {
    "name": "basic_env_exploration_heading",
    "collapsed": false
   },
   "source": "## Basic Env Exploration"
  },
  {
   "cell_type": "code",
   "id": "ba33c025-96bb-450b-8cd1-00abee631aed",
   "metadata": {
    "language": "python",
    "name": "table_listing"
   },
   "outputs": [],
   "source": "df_tables = session.sql(f'SHOW TABLES IN SCHEMA {database_name}.{schema_name}')\ndf_tables.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "21926bb1-b810-4eab-8098-feebb6941f5f",
   "metadata": {
    "name": "table_setup_heading",
    "collapsed": false
   },
   "source": "## Table Setup\nThis would be split into another notebook, but for demo purposes including here. Let's assume we have a stage table of employee listing and pay dates from our operational database or erp. Assumption is these are already setup in our staging environment in a src schema."
  },
  {
   "cell_type": "markdown",
   "id": "b4ea0655-fa7e-46a3-bdac-ac28aed41c4a",
   "metadata": {
    "name": "load_stage_env_heading",
    "collapsed": false
   },
   "source": "### Simulate Stage Env"
  },
  {
   "cell_type": "code",
   "id": "46eed5c3-7fce-4904-83b9-b5baf0a62478",
   "metadata": {
    "language": "python",
    "name": "src_schema_creation"
   },
   "outputs": [],
   "source": "session.sql(f\"\"\"\nCREATE SCHEMA IF NOT EXISTS {database_name}.{src_schema_name};\n\"\"\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4d4aa3b6-8b1e-45dd-b4ea-d75db84c3154",
   "metadata": {
    "language": "python",
    "name": "cell7"
   },
   "outputs": [],
   "source": "session.sql(f\"\"\"\nCREATE SCHEMA IF NOT EXISTS {database_name}.{etl_schema_name};\n\"\"\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f99f04f4-282c-44e0-9275-7ca45b5b474d",
   "metadata": {
    "language": "python",
    "name": "schema_listing",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "session.sql(f\"\"\"\nSHOW SCHEMAS IN {database_name};\n\"\"\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7b147db1-9132-41d2-9b70-385822248dc4",
   "metadata": {
    "language": "python",
    "name": "create_src_employee",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "session.sql(f\"\"\"\nCREATE TABLE {database_name}.{src_schema_name}.employee (\n    employee_id BIGINT,\n    name STRING,\n    department STRING\n);\n\"\"\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9f20459e-4380-4c90-87e2-f0390c9c3b2e",
   "metadata": {
    "language": "python",
    "name": "src_employee_load",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "session.sql(f\"\"\"\nINSERT INTO {database_name}.{src_schema_name}.employee (employee_id, name, department)\nVALUES\n    (1, 'Jack', 'Finance'),\n    (2, 'Jill', 'Merchandising')\n\"\"\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0fa5612c-c938-4483-b2f5-5d42b88ae481",
   "metadata": {
    "language": "python",
    "name": "src_employee_show",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "session.sql(f\"\"\"\nSELECT * FROM {database_name}.{src_schema_name}.employee\n\"\"\").show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f2144af0-1a79-4181-9890-c57c125d163f",
   "metadata": {
    "language": "python",
    "name": "create_src_employee_pay"
   },
   "outputs": [],
   "source": "session.sql(f\"\"\"\nCREATE TABLE {database_name}.{src_schema_name}.employee_pay (\n    employee_id BIGINT,\n    pay_date DATE,\n    pay_amount NUMBER(10,2)\n);\n\"\"\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "90da9d6c-1424-456d-83eb-7c4c258b8c82",
   "metadata": {
    "language": "python",
    "name": "employee_pay_load"
   },
   "outputs": [],
   "source": "session.sql(f\"\"\"\nINSERT INTO {database_name}.{src_schema_name}.employee_pay (employee_id, pay_date, pay_amount)\nVALUES\n    (1, '2025-09-30', 100.00),\n    (2, '2025-09-30', 200.00)\n\"\"\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "83336176-c408-44ad-a212-f63e64470a5b",
   "metadata": {
    "name": "create_tables_heading",
    "collapsed": false
   },
   "source": "## Create Tables\nEach cell would be its own file. Importantly, for the pattern to work primary keys on the natural composite key must be defined."
  },
  {
   "cell_type": "code",
   "id": "46e6e645-e36f-45ed-816f-6c1e6c1b27af",
   "metadata": {
    "language": "python",
    "name": "create_dim_employee",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "session.sql(f\"\"\"\nCREATE TABLE {database_name}.{schema_name}.dim_employee (\n    dim_employee_key BIGINT AUTOINCREMENT,\n    employee_id BIGINT,\n    name STRING,\n    department STRING,\n    etl_row_hash_value STRING,\n    create_username STRING,\n    create_datetime TIMESTAMP_NTZ,\n    last_update_username STRING,\n    last_update_datetime TIMESTAMP_NTZ\n);\n\"\"\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "50102381-6db5-4ab6-ab5f-d65d120f443c",
   "metadata": {
    "language": "python",
    "name": "establish_dim_employee_key",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "session.sql(f\"\"\"\nALTER TABLE {database_name}.{schema_name}.dim_employee\nADD CONSTRAINT pk_dim_employee PRIMARY KEY (employee_id);\n\"\"\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d0be5c5b-1620-47c0-b06a-be3ffd33178a",
   "metadata": {
    "language": "python",
    "name": "show_dim_employee",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "session.sql(f\"\"\"\nDESCRIBE TABLE {database_name}.{schema_name}.dim_employee\n\"\"\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "abc31a5d-e368-49bd-b7b9-512527ce8745",
   "metadata": {
    "language": "python",
    "name": "create_fact_employee_pay",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Keep natural dim foreign keys (employee_id) for debugging if ever needed\nsession.sql(f\"\"\"\nCREATE TABLE {database_name}.{schema_name}.fact_employee_pay (\n    fact_employee_pay_key BIGINT AUTOINCREMENT,\n    dim_employee_key BIGINT,\n    pay_date DATE,\n    pay_amount NUMBER(10,2),\n    employee_id BIGINT,\n    etl_row_hash_value STRING,\n    create_username STRING,\n    create_datetime TIMESTAMP_NTZ,\n    last_update_username STRING,\n    last_update_datetime TIMESTAMP_NTZ\n);\n\"\"\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "39f0bc58-87b1-41c7-bc57-dbb37c2a2c14",
   "metadata": {
    "language": "python",
    "name": "establish_fact_employee_pay",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "session.sql(f\"\"\"\nALTER TABLE {database_name}.{schema_name}.fact_employee_pay\nADD CONSTRAINT pk_fact_employee_pay PRIMARY KEY (employee_id, pay_date);\n\"\"\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d281476f-2798-42d6-85de-df36434770e5",
   "metadata": {
    "language": "python",
    "name": "show_fact_employee_pay",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "session.sql(f\"\"\"\nDESCRIBE TABLE {database_name}.{schema_name}.fact_employee_pay\n\"\"\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "80b25d2c-5089-4734-9fdc-383cf49a7bf2",
   "metadata": {
    "name": "load_pattern_heading",
    "collapsed": false
   },
   "source": "## Load Pattern\nThis would be defined somewhere in a common access library file"
  },
  {
   "cell_type": "code",
   "id": "5065cdd3-1b7e-48dc-ae2f-de4b6820d0bc",
   "metadata": {
    "language": "python",
    "name": "loader_utility_class"
   },
   "outputs": [],
   "source": "class TableUpdater:\n    def __init__(\n        self,\n        table_name\n    ):\n        \"\"\"Updater class that upserts data from etl view to edw table\n\n        Args:\n            table_name (str): Name of the table (view name will be inferred)\n        \"\"\"\n\n        # Infer base object names and calc audit columns\n        self.table_name = table_name\n        self.current_username = 'demo_user' # in actual env: session.sql(\"SELECT CURRENT_USER() AS current_user\").collect()[0][0]\n        self.current_datetime_cst = session.sql(\"SELECT CONVERT_TIMEZONE('America/Los_Angeles', 'America/Chicago', CURRENT_TIMESTAMP())::TIMESTAMP_NTZ AS current_time_cst\").collect()[0][0]\n        self.full_table_name = f'{database_name}.{schema_name}.{self.table_name}'\n        self.etl_view_name = f'{database_name}.{etl_schema_name}.vw_{self.table_name}'\n        self.table_primary_key_column_name = f'{self.table_name}_key'\n        self.updates_table_name = f'{database_name}.{etl_schema_name}.{self.table_name}_updates'\n\n        # Full Column Listing\n        column_listing: list[Row] = session.sql(f\"\"\"\n        SELECT COLUMN_NAME\n        FROM INFORMATION_SCHEMA.COLUMNS\n        WHERE\n            TABLE_CATALOG = UPPER('{database_name}')\n        AND TABLE_SCHEMA = UPPER('{schema_name}')\n        AND TABLE_NAME = UPPER('{self.table_name}')\n        ORDER BY ORDINAL_POSITION;\n        \"\"\").collect()\n        column_listing = [row.COLUMN_NAME.lower() for row in column_listing]\n\n        # Determine primary keys and join strings\n        table_natural_keys_list: list[Row] = session.sql(f'SHOW PRIMARY KEYS IN TABLE {self.full_table_name}').collect()\n        self.table_natural_keys_list = [row.column_name.lower() for row in table_natural_keys_list]\n        self.natural_key_join_string = ' AND '.join([f'source.{natural_key_column_name} = target.{natural_key_column_name}' for natural_key_column_name in self.table_natural_keys_list])\n        self.update_table_columns = [column_name for column_name in column_listing if column_name not in (self.table_primary_key_column_name, 'create_username', 'create_datetime', 'last_update_username', 'last_update_datetime')]\n        self.insert_columns = [column_name for column_name in column_listing if column_name not in (self.table_primary_key_column_name)]\n        self.update_hash_columns = [column_name for column_name in column_listing if column_name not in (self.table_primary_key_column_name, 'create_username', 'create_datetime') and column_name not in self.table_natural_keys_list]\n\n\n    def identify_upserts(self):\n        sql_string = f\"\"\"\n        CREATE OR REPLACE TABLE {self.updates_table_name} AS \n        SELECT\n             target.{self.table_primary_key_column_name}\n            ,{','.join([f'source.{col}' for col in self.update_table_columns])}\n            ,'{self.current_username}' as create_username\n            ,CAST('{self.current_datetime_cst}' AS TIMESTAMP_NTZ) as create_datetime\n            ,'{self.current_username}' as last_update_username\n            ,CAST('{self.current_datetime_cst}' AS TIMESTAMP_NTZ) as last_update_datetime\n            ,CASE \n                WHEN target.{self.table_primary_key_column_name} IS NULL THEN 'insert'\n                ELSE 'update'\n             END as insert_update_indicator\n        FROM {self.etl_view_name} source\n        LEFT JOIN {self.full_table_name} target\n            ON {self.natural_key_join_string}\n        WHERE\n            target.{self.table_primary_key_column_name} IS NULL\n        OR source.etl_row_hash_value <> target.etl_row_hash_value\n        \"\"\"\n        print(sql_string)\n        execution_results = session.sql(sql_string)\n        execution_results.show()\n\n        change_audit_sql_string = f\"\"\"\n        SELECT\n             SUM(CASE WHEN insert_update_indicator = 'insert' THEN 1 ELSE 0 END) AS new_records\n            ,SUM(CASE WHEN insert_update_indicator = 'update' THEN 1 ELSE 0 END) AS change_records\n        FROM {self.updates_table_name}\n        \"\"\"\n        print(change_audit_sql_string)\n        session.sql(change_audit_sql_string).show()\n\n\n    def process_table_updates(self):\n        \n        sql_string = f\"\"\"\n        MERGE INTO {self.full_table_name} as target\n        USING {self.updates_table_name} as source\n        ON source.{self.table_primary_key_column_name} = target.{self.table_primary_key_column_name}\n        WHEN MATCHED AND source.insert_update_indicator = 'update'\n        THEN UPDATE SET\n        {', '.join([f'target.{column_name} = source.{column_name}' for column_name in self.update_hash_columns])}\n        \"\"\"\n        print(sql_string)\n        execution_results = session.sql(sql_string)\n        execution_results.show()\n\n\n    def process_table_inserts(self):\n        \n        sql_string = f\"\"\"\n        MERGE INTO {self.full_table_name} as target\n        USING {self.updates_table_name} as source\n        ON {self.natural_key_join_string}\n        WHEN NOT MATCHED AND source.insert_update_indicator = 'insert'\n        THEN INSERT ({', '.join(self.insert_columns)})\n        VALUES ({', '.join([f'source.{col}'for col in self.insert_columns])})\n        \"\"\"\n        print(sql_string)\n        execution_results = session.sql(sql_string)\n        execution_results.show()\n        \n        \n        ",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "23f83bee-0b42-4051-b66e-f6b61b5bdb79",
   "metadata": {
    "name": "load_examples_heading",
    "collapsed": false
   },
   "source": "## Load Examples\nEach of these would be their own notebook"
  },
  {
   "cell_type": "markdown",
   "id": "747506e3-7432-473d-b32b-c4c22dd2fde4",
   "metadata": {
    "name": "dim_employee_load_heading",
    "collapsed": false
   },
   "source": "### dim_employee"
  },
  {
   "cell_type": "code",
   "id": "dbe61c2d-8ffc-47a1-a175-3d3a019f89a1",
   "metadata": {
    "language": "python",
    "name": "vw_dim_employee",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "session.sql(f\"\"\"\nCREATE OR REPLACE VIEW {database_name}.{etl_schema_name}.vw_dim_employee AS\nSELECT\n     employee_id\n    ,name\n    ,department\n    ,SHA1(CONCAT_WS('|',\n        COALESCE(CAST(name as STRING), '|'),\n        COALESCE(CAST(department as STRING), '|')\n    )) AS etl_row_hash_value --all but natural composite key\nFROM {database_name}.{src_schema_name}.employee\n\"\"\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "14005bb9-647f-44d0-bcec-4750c3c19fe9",
   "metadata": {
    "language": "python",
    "name": "show_vw_dim_employee",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "session.sql(f\"\"\"\nSELECT * FROM {database_name}.{etl_schema_name}.vw_dim_employee\n\"\"\").show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "11f261d2-91b3-4831-bee4-b660e35a5dcc",
   "metadata": {
    "language": "python",
    "name": "instantiate_dim_employee_updater",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "table_updater = TableUpdater(table_name = 'dim_employee')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f47bba79-5b05-4df6-8c8e-c9f092aa47e7",
   "metadata": {
    "language": "python",
    "name": "dim_employee_update_table",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "table_updater.identify_upserts()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "650ffe88-b275-4dee-bdee-e4dde175ef76",
   "metadata": {
    "language": "python",
    "name": "dim_employee_updates"
   },
   "outputs": [],
   "source": "table_updater.process_table_updates()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "013ede13-5a19-486b-98b2-cc0de6949f4e",
   "metadata": {
    "language": "python",
    "name": "dim_employee_inserts",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "table_updater.process_table_inserts()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "503adf57-96a1-4ed0-8331-e6ed37704308",
   "metadata": {
    "name": "fact_employee_pay_load_heading",
    "collapsed": false
   },
   "source": "### fact_employee_pay"
  },
  {
   "cell_type": "code",
   "id": "2976bf1e-2269-4635-982d-c54479d46407",
   "metadata": {
    "language": "python",
    "name": "vw_fact_employee_pay"
   },
   "outputs": [],
   "source": "session.sql(f\"\"\"\nCREATE OR REPLACE VIEW {database_name}.{etl_schema_name}.vw_fact_employee_pay AS\nSELECT\n     e.dim_employee_key\n    ,p.pay_date\n    ,p.pay_amount\n    ,p.employee_id\n    ,SHA1(CONCAT_WS('|',\n        COALESCE(CAST(p.pay_amount as STRING), '|')\n    )) AS etl_row_hash_value --all but natural composite key\nFROM {database_name}.{src_schema_name}.employee_pay p\nINNER JOIN {database_name}.{schema_name}.dim_employee e\n    ON p.employee_id = e.employee_id\n\"\"\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "57962b93-233a-477d-9a07-cbebb5b1a9a4",
   "metadata": {
    "language": "python",
    "name": "show_vw_fact_employee_pay",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "session.sql(f\"\"\"\nSELECT * FROM {database_name}.{etl_schema_name}.vw_fact_employee_pay\n\"\"\").show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e2f95e9d-e6a9-41bb-bc65-47d99f5f413a",
   "metadata": {
    "language": "python",
    "name": "instantiate_fact_employee_pay_loader"
   },
   "outputs": [],
   "source": "table_updater = TableUpdater(table_name = 'fact_employee_pay')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "38e5ba04-bcfd-4f64-b90a-8e6d48e1d727",
   "metadata": {
    "language": "python",
    "name": "fact_employee_pay_update_table"
   },
   "outputs": [],
   "source": "table_updater.identify_upserts()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c4b16783-2507-4982-a8bf-306d91b55a0d",
   "metadata": {
    "language": "python",
    "name": "fact_employee_pay_updates"
   },
   "outputs": [],
   "source": "table_updater.process_table_updates()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "84085cbf-9c81-4631-add0-6796b36b9a43",
   "metadata": {
    "language": "python",
    "name": "fact_employee_pay_inserts"
   },
   "outputs": [],
   "source": "table_updater.process_table_inserts()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f941be3a-4b78-49af-950e-97c7b5fa3992",
   "metadata": {
    "name": "updates_testing_heading"
   },
   "source": "### fact_employee_pay - test updates\nManually modifying some source data, make sure updates are picked up."
  },
  {
   "cell_type": "code",
   "id": "5587e402-c99f-4fe0-b645-c4a735b908b3",
   "metadata": {
    "language": "python",
    "name": "cell2"
   },
   "outputs": [],
   "source": "session.sql(f\"\"\"\nUPDATE {database_name}.{src_schema_name}.employee_pay\nSET pay_amount = 150.00\nWHERE employee_id = 1\n\"\"\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "08dbea1b-3a33-4435-ae85-78dad477076f",
   "metadata": {
    "language": "python",
    "name": "show_difference"
   },
   "outputs": [],
   "source": "session.sql(f\"\"\"\nSELECT * FROM {database_name}.{etl_schema_name}.vw_fact_employee_pay\n\"\"\").show()\n\nsession.sql(f\"\"\"\nSELECT * FROM {database_name}.{schema_name}.fact_employee_pay\n\"\"\").show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d9c0bdd0-9fac-4f40-9980-e04ecb4b9791",
   "metadata": {
    "language": "python",
    "name": "update_test_instantiate",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "table_updater = TableUpdater(table_name = 'fact_employee_pay')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1bb8a4d2-691e-4653-a927-bc05f7ab7d61",
   "metadata": {
    "language": "python",
    "name": "update_test_build_upsert_table",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "table_updater.identify_upserts()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2b8b5347-f8df-4206-b39a-08f72ce659ca",
   "metadata": {
    "language": "python",
    "name": "test_updates"
   },
   "outputs": [],
   "source": "table_updater.process_table_updates()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8098fb35-306d-4b2c-86a6-b0e46b7aa993",
   "metadata": {
    "language": "python",
    "name": "test_inserts"
   },
   "outputs": [],
   "source": "table_updater.process_table_inserts()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a7d64c3c-0dce-4372-8dde-92712f71a1ed",
   "metadata": {
    "language": "python",
    "name": "prove_update"
   },
   "outputs": [],
   "source": "session.sql(f\"\"\"\nSELECT * FROM {database_name}.{etl_schema_name}.vw_fact_employee_pay\n\"\"\").show()\n\nsession.sql(f\"\"\"\nSELECT * FROM {database_name}.{schema_name}.fact_employee_pay\n\"\"\").show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "69114a79-c349-4e83-9fb5-95347231ce22",
   "metadata": {
    "name": "end_summary",
    "collapsed": false
   },
   "source": "# Summary\nThis was a high level demonstration of a pattern that could be used in snowflake for updating fact and dim tables from source. As you can see in the last few sections, it's dynamic in a way that the focus is on architecture and business logic (view definition) for any custom code where all inserts/updates are handled dynamically.\n\nHowever, this is just a start. Here are some additional topics that this pattern could be extended to handle:\n1. Deletes: A method can be added to join the final fact table back to the view and delete any records that exist in the table but not the view. This would only be done with facts where a delete is appropriate and the view has the full dataset available.\n2. Aggregates: Simple class could be added to do an aggregate update - given a view that queries facts/dims, build an aggregate either in full or dynamic view a date range that looks at updates recently made in all the facts.\n3. Error handling/asserts: For production uses, the class should do things like check if tables/views exist, primary keys are defined, etc. In this example, I knew what I wanted to do - but having helpful assert error messages can assist the team in debugging quicker.\n4. Email alerts/data integrity: Checking for various scenarios and flagging if they occur during table loads.\n5. Sourcing data: Need a process to build out that src stage from true data rather than simulated records.\n6. Dedup/source data quality: Source data should also have a natural key defined and deduplicate/warn if there are violations.\n7. Type 2 dims: Usually try not to do this if possible, but there may be cases where type 2 dims make sense. The pattern could be expanded to keep track of a row effective and expiration date. Instead of doing an update, it would enddate the matching record and do an insert.\n8. Probably a bunch of other stuff I'm forgetting at the moment!"
  }
 ]
}